# -*- coding: utf-8 -*-
"""HPCA_Ass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p24XKk8DOFwqb2MpC0htb-U4rgmc57c9
"""

#importing the necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,r2_score,mean_squared_error,mean_absolute_error
import numpy as np


#ulpading files in the google-colab 
from google.colab import files

# uploading data file to google colab
uploaded = files.upload()

import io

train_df = pd.read_csv(io.BytesIO(uploaded['deep_r_train.csv']))

from sklearn import linear_model,preprocessing

train_df.shape  # getting the dimensions of the train dataset.
train_df['CPI'] = train_df["cycles"]/train_df["instructions"]  #creatung a new column 'CPI' which contains clocks per instruction value for each row.
result = train_df.copy()
result.head() #printing the first few rows of the dataset

#np.average(CPI)

#applying the min-max normalisation on each column
for feature_name in ["L1-dcache-load-misses","iTLB-load-misses","L1-icache-load-misses","LLC-store-misses","branch-misses:u","LLC-load-misses","dTLB-load-misses","dTLB-store-misses","L2-load-misses","L2-store-misses","ns"]	:
  max_value = train_df[feature_name].max()
  min_value = train_df[feature_name].min()
  result[feature_name] = (train_df[feature_name] - min_value) / (max_value - min_value)


result.head() #printing the first few rows of the dataset


#plotting a few data points to visualise the distribution
result.plot(kind='scatter',x='iTLB-load-misses',y='ns')
plt.show()


# Y = normalised 'ns' column and X = different feature variables in normalised form
Y = result.iloc[:,-2]
X = result.iloc[:,0:10]


#creating cross-validation set by randomly taking 20% of the total collected data.
X_train,X_test,Y_train,Y_test =  train_test_split(X,Y,test_size=0.20,random_state=42)


#invoking the python library SGDRegressor() model and subsequetly fitting the train data set.
model = linear_model.SGDRegressor()
model.fit(X_train, Y_train)


#printing the coefficients of various features after training is done 
i = 0
for feature_name in ["L1-dcache-load-misses","iTLB-load-misses","L1-icache-load-misses","LLC-store-misses","branch-misses:u","LLC-load-misses","dTLB-load-misses","dTLB-store-misses","L2-load-misses","L2-store-misses"]:
  print(feature_name,(model.coef_)[i])
  i = i + 1

print("intercept",model.intercept_[0])



#y_pred = model.predict(X_test)
#print(mean_absolute_error(Y_test,y_pred))
#print(mean_squared_error(Y_test, y_pred))

#obtaining the accuracy score for the cross-validation set upon the trained model
print(model.score(X_test,Y_test))


#uplodaing the test dataset .csv file in google colab 
uploaded_2 = files.upload()

test_df = pd.read_csv(io.BytesIO(uploaded_2['deep_r_test.csv']))

n=(test_df.shape)[0]
k = 10
print(n)
res = test_df.copy()

#calculating the average CPI 
CPI = res["cycles"]/res["instructions"]
CPI.ravel()
print("CPI(test)",np.average(CPI)) 	# printing the average value of CPI


#normalising the features using Min-max normalisation technique
for feature_name in ["L1-dcache-load-misses","iTLB-load-misses","L1-icache-load-misses","LLC-store-misses","branch-misses:u","LLC-load-misses","dTLB-load-misses","dTLB-store-misses","L2-store-misses","L2-load-misses","ns"]	:
  max_value = test_df[feature_name].max()
  min_value = test_df[feature_name].min()
  res[feature_name] = (test_df[feature_name] - min_value) / (max_value - min_value)
res.head()

Y_valid = res.iloc[:,-1]	# extracting the target values
X_valid = res.iloc[:,0:10]	# extracting the feature variables 



#calculating the various parametrs such as residuals, r^2, adj r^2, F-stat, p_val etc.  
from math import sqrt
y_pred = model.predict(X_valid)
print("mean-abs-err",mean_absolute_error(Y_valid,y_pred))
print("RSME",sqrt(mean_squared_error(Y_valid, y_pred)))



# computation with formulas from the theory
SSE = sum((Y_valid-y_pred)**2)       #RSS residual sum of squares
SS_Total = sum((Y_valid-np.mean(Y_valid))**2) #Variance of target    
r_squared = 1 - (float(SSE))/SS_Total		# r^2 value
adjusted_r_squared = 1 - (1-r_squared)*(len(Y_valid)-1)/(len(Y_valid)-X_valid.shape[1]-1)   # adj r^2 value

Rsq = model.score(X_valid,Y_valid)   #Root mean square
F_stat = (Rsq/(1-Rsq))*((n-k-1)/k)    #F-statistic
print("RSS",SSE)
print("var-of-predicted-vals",sum((y_pred - np.mean(y_pred))**2)/n)
print("var-of-target",SS_Total/n)
print("R-squared",r_squared)
print("adj-r-sq",adjusted_r_squared)
print("F-stat",F_stat)



import symbulate as sm

dfN = n-k-1 #degrees of freedom in the numerator of F-statistic
dfD = k-1 #degrees of freedom in the denominator of F-statistic

pVal = 1-sm.F(dfN,dfD).cdf(F_stat)
print("pVal",pVal)

!pip install symbulate
